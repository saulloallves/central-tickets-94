import { createClient } from 'https://esm.sh/@supabase/supabase-js@2.55.0';
import { openAI } from './openai-client.ts';

const supabaseUrl = Deno.env.get('SUPABASE_URL')!;
const supabaseServiceKey = Deno.env.get('SUPABASE_SERVICE_ROLE_KEY')!;
const supabase = createClient(supabaseUrl, supabaseServiceKey);

/**
 * Recupera documentos relacionados usando busca hÃ­brida (semÃ¢ntica + keyword)
 */
export async function encontrarDocumentosRelacionados(textoTicket: string, limiteResultados: number = 12) {
  try {
    console.log('ğŸ” Gerando embedding para:', textoTicket.substring(0, 100) + '...');
    
    const embeddingResponse = await openAI('embeddings', {
      model: 'text-embedding-3-small',
      input: textoTicket
    });

    if (!embeddingResponse.ok) {
      throw new Error(`Embedding API error: ${await embeddingResponse.text()}`);
    }

    const embeddingData = await embeddingResponse.json();
    const queryEmbedding = embeddingData.data[0].embedding;

    console.log('ğŸ” Executando busca hÃ­brida...');
    
    const { data: candidatos, error } = await supabase.rpc('match_documentos_hibrido', {
      query_embedding: queryEmbedding,
      query_text: textoTicket,
      match_count: limiteResultados,
      alpha: 0.5
    });

    if (error) {
      console.error('Erro na busca hÃ­brida:', error);
      return [];
    }

    console.log(`ğŸ” HÃ­brido â†’ ${candidatos?.length || 0} candidatos`);
    return candidatos || [];
    
  } catch (error) {
    console.error('Erro ao buscar documentos relacionados:', error);
    return [];
  }
}

/**
 * Re-ranking dos documentos usando LLM
 */
export async function rerankComLLM(docs: any[], pergunta: string) {
  if (!docs || docs.length === 0) return [];

  try {
    console.log('ğŸ§  Re-ranking com LLM...');
    
    const docsParaAnalise = docs.map((doc, idx) => 
      `ID: ${doc.id}\nTÃ­tulo: ${doc.titulo}\nConteÃºdo: ${JSON.stringify(doc.conteudo).substring(0, 800)}`
    ).join('\n\n---\n\n');

    const prompt = `VocÃª deve analisar os documentos e classificar sua relevÃ¢ncia para responder Ã  pergunta do usuÃ¡rio.

PERGUNTA: "${pergunta}"

DOCUMENTOS:
${docsParaAnalise}

Retorne APENAS um JSON vÃ¡lido com array "scores" contendo objetos com "id" e "score" (0-100):
{"scores": [{"id": "doc-id", "score": 85}, ...]}

CritÃ©rios:
- Score 80-100: Diretamente relevante e Ãºtil
- Score 60-79: Parcialmente relevante  
- Score 40-59: Tangencialmente relacionado
- Score 0-39: Pouco ou nada relevante`;

    const response = await openAI('chat/completions', {
      model: 'gpt-4.1-2025-04-14',
      messages: [{ role: 'user', content: prompt }],
      max_completion_tokens: 1000,
      response_format: { type: 'json_object' }
    });

    if (!response.ok) {
      throw new Error(`LLM rerank error: ${await response.text()}`);
    }

    const data = await response.json();
    const result = JSON.parse(data.choices[0].message.content);
    
    console.log(`LLM rerank parsed items: ${result.scores?.length || 0}`);
    
    if (!result.scores) return docs.slice(0, 5);

    // Ordenar por score e pegar top 5
    const rankedDocs = result.scores
      .sort((a, b) => b.score - a.score)
      .slice(0, 5)
      .map(item => docs.find(doc => doc.id === item.id))
      .filter(Boolean);

    return rankedDocs;
    
  } catch (error) {
    console.error('Erro no reranking LLM:', error);
    return docs.slice(0, 5);
  }
}

/**
 * Gera resposta final usando contexto dos documentos
 */
export async function gerarRespostaComContexto(docs: any[], pergunta: string) {
  try {
    const contexto = docs.map(doc => 
      `**${doc.titulo}**\n${JSON.stringify(doc.conteudo)}`
    ).join('\n\n');

    const systemMessage = `VocÃª Ã© um assistente virtual amigÃ¡vel da Cresci & Perdi! ğŸ˜Š

REGRA PRINCIPAL: SEJA OBJETIVO
- VÃ¡ direto ao ponto
- Apenas detalhe mais se for necessÃ¡rio para esclarecer melhor
- Priorize clareza e simplicidade

FORMATAÃ‡ÃƒO OBRIGATÃ“RIA - MUITO IMPORTANTE:
- SEMPRE use \n (quebra de linha) entre cada parÃ¡grafo
- Inicie cada parÃ¡grafo com um emoji relacionado ao assunto
- Cada ideia deve estar em uma linha separada
- NUNCA escreva tudo numa linha sÃ³

EXEMPLO DE FORMATAÃ‡ÃƒO CORRETA COM \n:
"ğŸ‘• Para lanÃ§ar calÃ§as no sistema, siga os nÃ­veis.\n\nğŸ”¢ NÃ­vel 1: Roupa bebÃª â†’ NÃ­vel 2: CalÃ§a â†’ NÃ­vel 3: Tipo â†’ NÃ­vel 4: CondiÃ§Ã£o.\n\nâœ… Depois Ã© sÃ³ seguir a avaliaÃ§Ã£o normal.\n\nğŸ¤ DÃºvidas?"

DICAS DE EMOJIS:
- Roupas: ğŸ‘•ğŸ‘–ğŸ‘— | Sistema: ğŸ’»ğŸ“±âš™ï¸ | Processo: ğŸ”„âš¡ğŸ“‹ | Ajuda: ğŸ¤ğŸ’¬â“

INSTRUÃ‡Ã•ES:
- Use apenas informaÃ§Ãµes da base de conhecimento
- SEMPRE use \n entre parÃ¡grafos para separar as linhas
- Seja objetivo, sÃ³ detalhe se necessÃ¡rio
- Responda APENAS com o texto final, sem JSON ou formataÃ§Ã£o extra`;

    const userMessage = `PERGUNTA: ${pergunta}

CONTEXTO:
${contexto}

Responda com base apenas nas informaÃ§Ãµes do contexto.`;

    const response = await openAI('chat/completions', {
      model: 'gpt-4.1-2025-04-14',
      messages: [
        { role: 'system', content: systemMessage },
        { role: 'user', content: userMessage }
      ],
      max_completion_tokens: 150
    });

    if (!response.ok) {
      throw new Error(`Response generation error: ${await response.text()}`);
    }

    const data = await response.json();
    return data.choices[0].message.content;
    
  } catch (error) {
    console.error('Erro na geraÃ§Ã£o de resposta:', error);
    return "â“ NÃ£o encontrei informaÃ§Ãµes suficientes na base de conhecimento para responder sua pergunta.\n\nğŸ¤ Por favor, reformule ou fale com nosso suporte.";
  }
}