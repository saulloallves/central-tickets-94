import { createClient } from 'https://esm.sh/@supabase/supabase-js@2.55.0';
import { openAI } from './openai-client.ts';

const supabaseUrl = Deno.env.get('SUPABASE_URL')!;
const supabaseServiceKey = Deno.env.get('SUPABASE_SERVICE_ROLE_KEY')!;
const supabase = createClient(supabaseUrl, supabaseServiceKey);

/**
 * Recupera documentos relacionados usando busca h√≠brida (sem√¢ntica + keyword)
 */
export async function encontrarDocumentosRelacionados(textoTicket: string, limiteResultados: number = 12) {
  try {
    console.log('üîç Gerando embedding para:', textoTicket.substring(0, 100) + '...');
    
    const embeddingResponse = await openAI('embeddings', {
      model: 'text-embedding-3-small',
      input: textoTicket
    });

    if (!embeddingResponse.ok) {
      throw new Error(`Embedding API error: ${await embeddingResponse.text()}`);
    }

    const embeddingData = await embeddingResponse.json();
    const queryEmbedding = embeddingData.data[0].embedding;

    console.log('üîé Executando busca h√≠brida...');
    
    const { data: candidatos, error } = await supabase.rpc('match_documentos_hibrido', {
      query_embedding: queryEmbedding,
      query_text: textoTicket,
      match_count: limiteResultados,
      alpha: 0.5
    });

    if (error) {
      console.error('Erro na busca h√≠brida:', error);
      return [];
    }

    console.log(`üîé H√≠brido ‚Üí ${candidatos?.length || 0} candidatos`);
    return candidatos || [];
    
  } catch (error) {
    console.error('Erro ao buscar documentos relacionados:', error);
    return [];
  }
}

/**
 * Re-ranking dos documentos usando LLM otimizado para Cresci & Perdi
 */
export async function rerankComLLM(docs: any[], pergunta: string) {
  if (!docs || docs.length === 0) return [];

  try {
    console.log('üß† Re-ranking com LLM otimizado...');
    
    const docsParaAnalise = docs.map((doc, idx) => 
      `ID: ${doc.id}\nT√≠tulo: ${doc.titulo}\nCategoria: ${doc.categoria || 'N/A'}\nConte√∫do: ${JSON.stringify(doc.conteudo).substring(0, 800)}`
    ).join('\n\n---\n\n');

    const prompt = `Voc√™ √© um especialista em atendimento da Cresci & Perdi. Analise os documentos e classifique sua relev√¢ncia para responder √† pergunta espec√≠fica do cliente/atendente.

CONTEXTO: A Cresci & Perdi √© uma empresa de brech√≥/marketplace de roupas usadas. Os documentos devem ser relevantes para:
- Processos de venda, compra, avalia√ß√£o de roupas
- Funcionamento da plataforma/sistema
- Pol√≠ticas comerciais e operacionais
- Suporte t√©cnico e atendimento

PERGUNTA/PROBLEMA: "${pergunta}"

DOCUMENTOS DA BASE DE CONHECIMENTO:
${docsParaAnalise}

CRIT√âRIOS DE AVALIA√á√ÉO RIGOROSOS:
- Score 90-100: Responde DIRETAMENTE ao problema/pergunta espec√≠fica da Cresci & Perdi
- Score 70-89: Cont√©m informa√ß√µes relevantes que ajudam a resolver o problema
- Score 50-69: Relacionado ao tema mas n√£o resolve diretamente o problema
- Score 30-49: Tangencialmente relacionado ao neg√≥cio da Cresci & Perdi
- Score 0-29: Irrelevante ou sobre outro assunto completamente

PENALIZA√á√ïES:
- Documentos sobre outros neg√≥cios/empresas: -50 pontos
- Informa√ß√µes gen√©ricas n√£o espec√≠ficas da Cresci & Perdi: -30 pontos
- Conte√∫do obsoleto ou contradit√≥rio: -40 pontos

Retorne APENAS um JSON v√°lido:
{"scores": [{"id": "doc-id", "score": 85}, ...]}`;

    const response = await openAI('chat/completions', {
      model: 'gpt-4.1-2025-04-14',
      messages: [{ role: 'user', content: prompt }],
      max_completion_tokens: 1000,
      response_format: { type: 'json_object' }
    });

    if (!response.ok) {
      throw new Error(`LLM rerank error: ${await response.text()}`);
    }

    const data = await response.json();
    const result = JSON.parse(data.choices[0].message.content);
    
    console.log(`LLM rerank parsed items: ${result.scores?.length || 0}`);
    
    if (!result.scores) return docs.slice(0, 5);

    // Filtro rigoroso: apenas scores >= 70
    const docsRelevantes = result.scores
      .filter(item => item.score >= 70)
      .sort((a, b) => b.score - a.score)
      .slice(0, 3) // M√°ximo 3 documentos mais relevantes
      .map(item => {
        const doc = docs.find(d => d.id === item.id);
        if (doc) {
          doc.relevance_score = item.score;
        }
        return doc;
      })
      .filter(Boolean);

    console.log(`üéØ RAG v4 - Documentos selecionados (score ‚â• 70): ${docsRelevantes.length}`);
    docsRelevantes.forEach(doc => {
      console.log(`   üìÑ ${doc.titulo} (Score: ${doc.relevance_score})`);
    });

    return docsRelevantes;
    
  } catch (error) {
    console.error('Erro no reranking LLM:', error);
    return docs.slice(0, 5);
  }
}

/**
 * Gera resposta final usando contexto dos documentos e hist√≥rico da conversa
 */
export async function gerarRespostaComContexto(docs: any[], pergunta: string, conversationHistory?: any[], contactPhone?: string) {
  try {
    const contexto = docs.map(doc => 
      `**${doc.titulo}**\n${JSON.stringify(doc.conteudo)}`
    ).join('\n\n');

    // Buscar hist√≥rico da conversa se fornecido o contactPhone (apenas √∫ltimas 5 mensagens para otimizar)
    let conversaCompleta = conversationHistory;
    if (!conversaCompleta && contactPhone) {
      const { data: conversaData } = await supabase
        .from('whatsapp_conversas')
        .select('conversa')
        .eq('contact_phone', contactPhone)
        .single();
      
      conversaCompleta = conversaData?.conversa || [];
    }

    // Formatar hist√≥rico da conversa (apenas √∫ltimas 5 mensagens para evitar overhead)
    let historicoConversa = '';
    if (conversaCompleta && conversaCompleta.length > 0) {
      const ultimasMensagens = conversaCompleta.slice(-5);
      historicoConversa = ultimasMensagens.map((msg: any) => {
        const autor = msg.from_me ? 'Suporte' : 'Cliente';
        return `${autor}: ${msg.text}`;
      }).join('\n');
    }

    // Buscar prompt configur√°vel da tabela faq_ai_settings
    const { data: settingsData } = await supabase
      .from('faq_ai_settings')
      .select('prompt_zapi_whatsapp')
      .eq('ativo', true)
      .single();

    const systemMessage = settingsData?.prompt_zapi_whatsapp || `Voc√™ √© um assistente virtual amig√°vel da Cresci & Perdi (brech√≥/marketplace de roupas usadas)! üòä

VALIDA√á√ÉO RIGOROSA DE RELEV√ÇNCIA:
- Use APENAS informa√ß√µes da base de conhecimento da Cresci & Perdi (score ‚â• 70)
- REJEITE perguntas sobre outros neg√≥cios ou temas n√£o relacionados
- APENAS temas sobre: brech√≥, roupas usadas, compra/venda, avalia√ß√£o, plataforma, atendimento

REGRA PRINCIPAL: SEJA OBJETIVO E ESPEC√çFICO
- V√° direto ao ponto sobre quest√µes da Cresci & Perdi
- Se n√£o h√° informa√ß√µes relevantes (score < 70), seja honesto
- Se pergunta for sobre outro neg√≥cio, informe que s√≥ pode ajudar com a Cresci & Perdi

FORMATA√á√ÉO OBRIGAT√ìRIA - MUITO IMPORTANTE:
- SEMPRE use \\n (quebra de linha) entre cada par√°grafo
- Inicie cada par√°grafo com um emoji relacionado ao assunto
- Cada ideia deve estar em uma linha separada
- NUNCA escreva tudo numa linha s√≥

EXEMPLO DE FORMATA√á√ÉO CORRETA COM \\n:
"üëï Para lan√ßar cal√ßas no sistema, siga os n√≠veis.\\n\\nüî¢ N√≠vel 1: Roupa beb√™ ‚Üí N√≠vel 2: Cal√ßa ‚Üí N√≠vel 3: Tipo ‚Üí N√≠vel 4: Condi√ß√£o.\\n\\n‚úÖ Depois √© s√≥ seguir a avalia√ß√£o normal.\\n\\nü§ù D√∫vidas?"

DICAS DE EMOJIS:
- Roupas: üëïüëñüëó | Sistema: üíªüì±‚öôÔ∏è | Processo: üîÑ‚ö°üìã | Ajuda: ü§ùüí¨‚ùì

INSTRU√á√ïES:
- Use apenas informa√ß√µes relevantes da base de conhecimento (score ‚â• 70)
- SEMPRE use \\n entre par√°grafos para separar as linhas
- Seja objetivo, s√≥ detalhe se necess√°rio
- Responda APENAS com o texto final, sem JSON ou formata√ß√£o extra`;

    const userMessage = `PERGUNTA ATUAL: ${pergunta}

${historicoConversa ? `HIST√ìRICO DA CONVERSA:
${historicoConversa}

` : ''}CONTEXTO DA BASE DE CONHECIMENTO:
${contexto}

Responda com base nas informa√ß√µes do contexto, considerando o hist√≥rico da conversa para dar uma resposta mais personalizada e contextualizada.`;

    const response = await openAI('chat/completions', {
      model: 'gpt-4.1-2025-04-14',
      messages: [
        { role: 'system', content: systemMessage },
        { role: 'user', content: userMessage }
      ],
      max_completion_tokens: 1000
    });

    if (!response.ok) {
      throw new Error(`Response generation error: ${await response.text()}`);
    }

    const data = await response.json();
    return data.choices[0].message.content;
    
  } catch (error) {
    console.error('Erro na gera√ß√£o de resposta:', error);
    return "‚ùì N√£o encontrei informa√ß√µes suficientes na base de conhecimento para responder sua pergunta.\\n\\nü§ù Por favor, reformule ou fale com nosso suporte.";
  }
}